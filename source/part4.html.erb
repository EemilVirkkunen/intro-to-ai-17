---
  title: Part 4
  exercise_page: true
  quiz_page: false
  published: true
---




<% partial 'partials/exercise', locals: { name: 'Bayesian networks (pencil-and-paper) (1p)' } do %>

<p>
  In Part 3, we briefly discussed how Bayesian networks lead to compact
  representation of probabilistic models.
  <ol>
    <li>
      Consider a model with N=4 variables, each of which has k possible
      values (0,1,...,k&ndash;1). How many different elementary events are there
      in the domain? One less is the number of probabilities that we'd have
      to determine in order to fully specify the model. (For example, in the
      Car model of Part 3, with N=6 and k=2, this number was 63.)
    <li> 
      How many parameters would we have to define to specify the model using
      an "empty" Bayesian network, i.e, a network with no edges at all?
    <li>
      What is the maximum number of parameters we'd have to define in a
      Bayesian network in this domain (N variables with k values each)?
      Remember that the graph has to be a DAG, i.e., no directed cycles are
      allowed.
    <li>
      <i>Optional:</i> Generalize to arbitrary values of N. It may be helpful to
      recall the sum of a geometric series 1+k+k<sup>2</sup>+...+k<sup>N&ndash;1</sup> =
      (k<sup>N</sup>&ndash;1)/(k&ndash;1), where k&ne;1.
  </ol>
</p>

<% end %>

<% partial 'partials/exercise', locals: { name: 'Bayesian networks: Car (programming) (1p)' } do %>

<p>
  Implement an algorithm for generating data from the Car network
  discussed in Part 3.  You should generate n tuples, each of which is
  a six element list of bits (or Boolean values). You should start by
  choosing the value of B so that it takes value 1
  (or <code>True</code>) with probability 0.9. After choosing the
  value of B, choose the value of R. If B=0, then R=0 with probability
  1. If B=1, then R=0 with probability 0.1. Continue this way,
  choosing the probabilities from the CPTs, until you have generated a
  complete tuple.
</p>

<p>
  Generate a sample of n=100 000 tuples. Use it to approximate the following
  conditional probabilities:
  <ol>
    <li>
      P(B | R,G,&not;S)
    <li>
      P(S | R,I,G)
    <li>
      P(S | &not;R,I,G)
  </ol>
  <strong>
    and (don't forget this part):
  </strong>
  <ol start=4><br>
    <li>
      Give an interpretation to the above probabilities. Are they in line with
      your intuition? In other words, do they make sense?
  </ol>
</p>

<% end %>

<% partial 'partials/exercise', locals: { name: 'Bayesian networks: Earthquake (programming) (1p)' } do %>

<p>
  Consider the following scenario. You live in an area where
  earthquakes happen on the average every 111th day. Another annoying
  thing is that a burglar breaks into your home on the average once a
  year. Both events occur independently of each other and uniformly
  throughout the year (so any day is like another).
</p>

<p>
  In case of an earthquake, your home alarm goes off with probability 0.81.
  If your home is broken into, the alarm goes off with probability 0.92.
  If these both should occur at the same time, the alarm goes off with
  probability 0.97. If nothing special is going on, there's a false alarm
  with probability 0.0095.
</p>

<p>
  Now modify the algorithm from the previous exercise so that you can generate
  data from variables [E]arthquake, [B]urglar, [A]larm. Generate a sample of 
  n=100 000 tuples.
  <ol>
    <li>
      Among the tuples with A=1, what is the fraction where B=1?
    <li>
      Among the tuples with A=1 and E=1, what is the fraction where B=1?
  </ol>
  <strong>
    and (don't forget these):
  </strong>
  <ol start=3><br>
    <li>
      Give an interpretation to your answers for items 1 & 2. What
      probabilities do they approximate?
    <li>
      Are they in line with your intuition? In other words, do they
      make sense? In particular: which of the fractions is bigger and
      why?
    <li>
      Repeat the experiment a couple of times to get a feeling of how much
      the results change just by chance. (This is the nature of the Monte
      Carlo approximation.) Experiment with different values of n. How does
      it affect the variability?
  </ol>
</p>

<% end %>


You can find the lecture slides for this part on the <a href="https://courses.helsinki.fi/DATA15001/119123276">course homepage</a>
under Materials.


<% partial 'partials/material_heading' do %>
  Machine Learning
<% end %>

<p>
</p>

<% partial 'partials/hint', locals: { name: 'Learning objectives of Part 4' } do %>

<table class="table">
  <tr>
    <td>
      Theme
    </td>
    <td>
      Objectives (after the course, you ...)
    </td>
  </tr>
    <td>
      Machine learning
    </td>
    <td>
      <ul>
	<li>can distinguish between unsupervised and supervised machine learning scenarios
	<li>can implement at least two supervised classification methods (e.g., naive Bayes, nearest neighbour classifier)
	<li>know the main types of neural networks (feed-forward, recurrent, self-organizing map) and their main principles
	<li>can implement the perceptron algorithm in a simple binary classification problem
      </ul>
    </td>
  </tr>
</table>

<% end %>

<% partial 'partials/material_sub_heading' do %>
  Kinds of Machine Learning
<% end %>

<p>
  <ol>
    <li>Supervised learning
    <li>Unsupervised learning
    <li>Reinforcement learning
  </ol>
</p>

<p>
  We will focus primarily on supervised learning, and in particular,
  <b>classification tasks</b>. In classification, we observe in input,
  x, and try to infer its "class" y. There can be two or more possible
  class values. In the standard case, it is assumed that in each case,
  exactly one class value is "correct". In other words, it is not
  possible that an instance belongs to multiple classes (or none at
  all) at the same time. The <b>training data</b> is in the form of a
  set of (x,y) pairs.
</p>

<p><b>Classification error</b> means the fraction of incorrect
  classifications, i.e., cases where our classifier outputs the wrong
  class. It is important to keep in mind that the classification error
  can be quite different in the training data and a separate <b>test
    set</b>. This is related to the so called <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> phenomenon.
</p>

<p>
  Good examples of classification tasks include: spam filtering (input
  is set of words in the message, class is spam/ham) and handwritten
  digit recognition (input is a bitmap image, class is 0,...,9).
</p>
<img src="/img/diagrams/MNISTsamples.png" width=88%>

<p>
  <i>Figure:</i>&nbsp; Samples of handwritten images from the very
  commonly used <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>
  dataset. The correct label (decided by a human) is shown above each
  image. Note that some of the "correct" class labels are questionable:
  see for example the second image from left: is that really a '7',
  or actually a '4'?
</p>
  
<p>
  In this section, we'll next study two commonly used classifiers: the
  nearest neighbor classifier and the naive Bayes classifier (which is
  already familiar to you from the spam filter in Part 3).
</p>

<% partial 'partials/material_sub_heading' do %>
  k-Nearest Neighbor (k-NN) Classifier
<% end %>

<p>
  The <b>nearest neighbor</b> (NN) classifier is among the simplest
  possible classifiers. In the training stage, if you can call it
  that, all the training data is simply stored. When given a new
  instance, x<sup>(test)</sup>, we find the training data point
  x<sup>(NN)</sup> which is closest to x<sup>(test)</sup> and
  output its class label y<sup>(NN)</sup> as the predicted class
  label. An example is given in the following diagram (panels a and b).
</p>

<img src="/img/diagrams/nearestneighbors_diagram1.png" width=90%><br><br>

<p>
  In the k-nearest neighbor (k-NN) classifier, the idea is very
  similar but here it is not only the single nearest point that is
  used. Instead, we find the k nearest training data points, and
  choose the majority class among them as the predicted label.
  This is illustrated in panel c of the above diagram.
</p>

<% partial 'partials/hint', locals: { name: 'Faraway, So Close!' } do %>

<p>
  An interesting question related to (among other things) the NN
  classifier is the definition of <i>distance</i> or <i>similarity</i>
  between instances. In the illustration above, we implicitly assumed
  that the Euclidean distance is used. However, this may not always be
  reasonable or even possible: the type of the input x may be, e.g.,
  a string of characters (of possibly different lengths).</p>

<p>
  You should therefore choose the distance metric on a case-by-case
  basis. In the MNIST digit recognition case, we discussed the
  potential problems due to defining image similarity by counting
  pixel-by-pixel matches -- this was concluded to be sensitive to
  shifting or scaling the images. Fortunately, the MNIST data has
  been preprocessed by centering and scaling the images to alleviate
  some of these issues.
</p>
  
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Naive Bayes Classifier
<% end %>

<p>
  The naive Bayes classifier is actually already familiar to you from
  the spam filter problem in Part 3. Here we will see how the same
  idea can be applied to handwritten digit recognition. In the
  spam filter application, the class variable was binary (spam/ham) and
  the feature variables were the words in the message. In the
  digit recognition application, the class variable has ten
  different values (0,...,9), and the feature variables are the
  28 &times 28 = 784 pixel values (binary black/white) in the bitmap
  images. We store the pixel values in a single vector and use the 
  value X<sub>j</sub>=1 to indicate that the j'th pixel is white.
</p>

<p>
  Here too, the feature variables are assumed to be independent of
  each other given the class value, i = 0,...,9. In the training
  stage, the algorithm estimates the conditional probabilities of each
  pixel being white in an image of class i for each class and each
  pixel. In the same manner as in the spam filter case, these
  probabilities can be estimated as empirical frequencies:<br><br>
  P(X<sub>j</sub> = 1 | Class = i) = #{X<sub>j</sub>=1, Class=i} /
  #{Class=i},
  <br><br>
  where #{X<sub>j</sub> = 1, Class=i} is the number of training instances
  of class i where the j'th pixel is white, and #{Class=i} is the 
  overall number of class i instances.
</p>

<p>
  This estimator is implemented by the following pseudocode:
</p>

<% partial 'partials/code_highlight' do %>
train_NB(data):
1:  for each class i in 0,...,9:
2:     model.prior[i] = #{class=i} / n
3:     for each pixel j in 0,...,783: 
4:        model.cpt[i,j] = #{X<sub>j</sub>=1, class=i} / #{class=i}
5:  return model 
<% end %>

<p>
  The estimated values are stored as the array <code>cpt</code> in
  a <code>model</code> object. The array <code>prior</code> stores
  the estimates of the probabilities P(Class=i) for all i=0,...,9.
</p>

<p> Given a new image with pixel values
  x<sub>0</sub>,...,x<sub>783</sub> (so, for example, 0,0,0,1,... if the first
  three pixels are black and the fourth is white) the classifier computes 
  the joint probability<br><br> 
  P(Class=i, X<sub>0</sub>=x<sub>0</sub>,...,X<sub>783</sub>=x<sub>783</sub>) 
  = P(Class=i) P(X<sub>0</sub>=x<sub>0</sub> | Class=i) &times; ...
  &times; P(X<sub>783</sub>=x<sub>783</sub> | Class=i)
  <br><br> 
  for all i=0,...,9.
  The sum of these probabilities over the ten class values,
  Z, is the annoying denominator in the Bayes rule, so we can obtain
  the posterior probabilities by dividing the above joint
  probabilities by Z:<br><br>
  P(Class=i | X<sub>0</sub>=x<sub>0</sub>,...,X<sub>783</sub>=x<sub>783</sub>)
  = P(Class=i) P(X<sub>0</sub>=x<sub>0</sub> | Class=i) &times; ...
  &times; P(X<sub>783</sub>=x<sub>783</sub> | Class=i) / Z
</p>

<p>
  Pseudocode for doing computing the posterior probabilities is
  below.
</p>


<% partial 'partials/code_highlight' do %>
test(model, image):
1:  Z = 0; posterior = empty array 
2:  for each class i in 0,...,9:
3:     posterior[i] = model.p[i]
4:     for each feature j in 0,...,783: 
5:        if image[j] = 1:
6:           posterior[i] = posterior[i] * model.cpt[i,j]
7:        else:
8:           posterior[i] = posterior[i] * (1-model.cpt[i,j])
9:     Z = Z + posterior[i]
10: for each class i in 0,...,9:
11:    posterior[i] = posterior[i] / Z
12: return posterior
<% end %>  

<p>
  Note that since the pixels can only take one of two possible values
  (white or black), the probability of a pixel being black is obtained
  as one minus the probability of a white pixel (line 8 in the
  pseudocode).
</p>

<% partial 'partials/material_heading' do %>
  Neural Networks
<% end %>

<% partial 'partials/material_sub_heading' do %>
  What is Special About Neural Networks?
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Weights and Activation
<% end %>

<% partial 'partials/material_sub_heading' do %>
  The Perceptron Algorithm
<% end %>

<p>
  One classic neural network method is the Perceptron algorithm
  introduced by Rosenblatt in 1957.  A <b>perceptron</b> is a
  feedforward neural "network" that consists of a single basic
  neuron. It can be used as a simple classifier in binary
  classification tasks.
</p>

<p>
  In the algorithm, for which pseudocode is given below, each
  misclassification leads to an update in the parameter vector w.  If
  the predicted output of the neuron is 1 when the correct class is
  y=&ndash;1, then the input vector x is subtracted from the weight
  vector.  Similarly, if the predicted output is &ndash;1 when the
  correct class is y=1, then the input vector x is added to the weight
  vector. (Recall
  that <a href="https://www.mathsisfun.com/algebra/vectors.html">vector
  subtraction and addition</a> simply means the element-wise
  subtraction or addition of the two vectors.)
</p>

<% partial 'partials/code_highlight' do %>
perceptron(data): 
1:  w = [0, ...,0]               # array of size p
2:  while error(data, w) > 0:
3:     (x,y) = choose_random_item(data)
4:     z = w[0]x[0] + ... + w[p-1]x[p-1]  
5:     if z ≥ 0 and y = -1:      # -1 classified as 1 
6:        w = w − x              # subtract vector x
7:     if z < 0 and y = 1:       # 1 classified as -1
8:        w = w + x              # add vector x
9:  return(w)
<% end %>

<p>
  In practice, it is impractical to choose random training data
  points on line 3 of the algorithm because this may lead to choosing
  correctly labeled examples most of the time, which is a waste of
  time since they lead to no updates in the weights.  Instead, a
  better method is to iterate through the training data and as soon as
  a misclassified item is found, do the required update.
</p>

<p>
  It can be theoretically proven that if the data is <b>linearly
    separable</b>, then the algorithm is guaranteed to stop after a 
  finite number of steps and produce a weigth vector that correctly
  classifies all the training instances.
</p>

<p>
  After its discovery, the Perceptron algorithm raised a lot of
  attention, not least because of optimistic statements made by
  its inventor, Frank Rosenblatt.  A classic example of AI hyperbole
  is a New York Times article published on July 8th, 1958:
  <blockquote style="margin-left: 40px;">
    <p>The Navy revealed the embryo of an electronic computer today
      that it expects will be able to walk, talk, see, reproduce itself
      and be conscious of its existence. Later perceptrons will be able
      to recognize people and call out their names and instantly
      translate speech in one language to speech and writing in
      another language, it was predicted.</p>
  </blockquote>
  The history of the debate that eventually lead to almost complete
  abandoning of the neural network approach in the 1960s for more than
  two decades is extremely fascinating. The
  article <a href="https://doi.org/10.1177%2F030631296026003005">"A
  Sociological Study of the Official History of the Perceptrons
  Controversy"</a> by Mikel Olazaran (Social Studies of Science, 1996)
  reviews the events from a sociology of science point of view.
  Reading it today is quite thought provoking -- and slightly
  chilling. Take for example a September 29th
  2017 <a href="https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/">article
  in the MIT Technology Review</a>, where Jordan Jacobs, co-founder of
  a multimillion dollar <a href="http://vectorinstitute.ai/">Vector
  institute for AI</a> compares Geoffrey Hinton (who played a big part
  in the discovery of the power of the backpropagation algorithm in
  the 1980s, and is a figure-head of the current deep learning boom)
  to Einstein. 
</p>

<% partial 'partials/exercise', locals: { name: 'Perceptron (programming) (2p)' } do %>

<p>
  Use the Perceptron template on TMC. (You can also implement your solution
  in other languages than Java. You'll find the necessary data files in the
  TMC template.)
</p>

<p>
  The file <code>mnist-x.data</code> contains 6000 images from the popular
  <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a>, each
  of which is given on a single line in the file. Each image consists
  of 28 &times; 28 pixels listed row-by-row, so each line in the file
  contains 784 values.  Each pixel is either black (&ndash;1) or white
  (1). The file
  <code>mnist-y.data</code> contains the correct class value (0-9) of each
  of the 6000 images.
</p>

<p>
  Use the first 5000 images as training data and the last 1000 as test data.
  <ol>
    <li>
      Run the Java template to make sure that the file <code>test100.bmp</code>
      is created. It should include the first 100 images sorted according to
      the correct class label. This verifies that reading the file was
      successful.
    <li>
      Modify the <code>train()</code> method in
      class <code>Perceptron</code> so that it learns to distinguish
      number 3 from number 5. (Notice that the
      variable <code>targetChar</code> can be set to be one of these
      classes while <code>oppositeChar</code> should be the
      other. This will set the class label as 1 and &ndash;1 as is
      required in the Perceptron algorithm. Images representing other
      numbers are ignored.) Try to get a classification error around
      5&ndash;15 %.
    <li>
      Try other pairs of number than 3 vs 5. Which numbers are easiest to
      classify and which are the hardest?
  </ol>
<% end %>

<% partial 'partials/exercise', locals: { name: 'Nearest Neighbor (programming) (1p)' } do %>

<p>
  Now implement a nearest neighbor classifier for the MNIST data used in
  the Perceptron exercise.
</p>

<p>
  Given a test example, X<sup>(test)</sup>, the classifier will find the nearest
  training data point, X<sup>(NN)</sup>, and return its class label
  Y<sup>(NN)</sup>. Notice that unlike the Perceptron, or most other classifiers,
  the nearest neighbor classifier doesn't really involve a training stage.
  All the action happens in the classification (testing) stage.
</p>

<p>
  Test your classifier using the same train/test split (5000/1000) as before.
  You can use the same pairs of numbers (3 vs 5), or even try classifying all
  the classes at the same time because the NN classifier is not restricted
  to binary classification. (Note that in multiclass classification, the
  expected accuracy tends to be lower than in binary classification simply
  because the problem is harder.)
</p>

<% end %>

<p>
  The main problem with the Perceptron algorithm is the assumption
  that the data are linearly separable. In practice, this tends not to
  be the case, and various variants of the algorithm have been
  proposed to deal with this issue. Two main directions are:
  <ol>
    <li>
      Applying a non-linear transformation on the original input
      data may produce a representation where the data are 
      linearly separable. The so called "kernel trick" leads to
      a class of methods known collectively as <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel methods</a>, among which the
      support vector machine (SVM) is the best known.
    <li>
      The model can be extended by coupling a number of basic
      neurons together to obtain neural networks that can represent
      complex, non-linear decision boundaries. A classical example
      of this is the <b>multilayer perceptron</b> (MLP).
  </ol>
</p>

<p>
  Optimizing the weights of a multilayer perceptron is much harder than
  optimizing the weigths of a single perceptron neuron. The second
  coming of neural networks in the late 1980s was largely due to
  the difficulties faced by the then prevailing logic-based approach
  (so called expert systems) but also due to the invention of the
  <b>backpropagation algorithm</b> in the 1970s-1980s.
</p>

<% partial 'partials/hint', locals: { name: 'Meanwhile in Helsinki (1970)' } do %>

<p>
  The path(s) leading to the backpropagation algorithm are rather long
  and winding. An interesting part of the history is related to the CS
  department at the University of Helsinki. About three years after
  the founding of the department in
  1967, <a href="http://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf">a
  Master's thesis</a> was written by a student called Seppo
  Linnainmaa. The topic of the thesis was "Cumulative rounding errors
  of algorithms as a Taylor approximation of individual rounding
  errors" (the thesis was written in Finnish, so this is a translation
  of the actual title "Algorithmin kumulatiivinen py&oumL;ristysvirhe
  yksitt&auml;isten py&ouml;ristysvirheiden
  Taylor-kehitelm&auml;n&auml;").
</p>

<p>
  The automatic differentiation method developed in the thesis was
  later applied by other researchers to quantify the sensitivity of
  the output of a multilayer neural network with respect to the
  individual weights, which is the key idea in backpropagation.
</p>

<% end %>


<% partial 'partials/material_sub_heading' do %>
  Recurrent Neural Networks
<% end %>

<% partial 'partials/material_sub_heading' do %>
  The Self-Organizing Map (SOM)
<% end %>

