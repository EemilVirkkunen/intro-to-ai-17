---
  title: Part 3
  exercise_page: true
  quiz_page: false
  published: true
---


<% partial 'partials/exercise', locals: { name: 'Check & Mate! (2p)' } do %>

<p>
  Last week we practiced game algorithms, Minimax and alpha-beta
  pruning, on the simple game of tic-tac-toe. Tic-tac-toe is a good
  choice for understanding the basic principles and simulating the
  algorithms step-by-step. However, it is hardly a very cool
  AI application.
</p>

<p>
  Now we'll change gears and go for what used to be thought as a some
  kind of a "grand challenge" for AI, namely chess! (In fact, that's
  not exactly true since we'll be playing a variant of chess,
  called <a href="https://en.wikipedia.org/wiki/Los_Alamos_chess">Los
    Alamos chess</a> which is played on a 6 x 6 board with no bishops.)
</p>

<p>
  Implementing the complete game engine with all the rules is somewhat
  tedious, so we have done it for you. Also, since you've already
  implemented alpha-beta pruning last week, you don't have to do it
  again here. Instead, your task is to implement a <strong>
    heuristic evaluation function</strong>. The performance of the
  chess bot is in part determined by the goodness of the evaluation
  function, so designing a good one is critical for creating a
  competitive bot.
</p>

<p>
  To make things exciting, we'll have a <strong>chess-bot
  tournament</strong>. You will submit your solution (the heuristic
  evaluation function) on our server and compete against our own
  contender, <i>Deep Glue</i> and bots by other students. May the
  best bot win!
</p>

<ol>
  <li>
    Download the Java template <a href="https://materiaalit.github.io/intro-to-ai-17/fi\
les/Chesspackage.zip"></a>here.
  <li>
    Implement the method <code>double eval(Position p)</code> in class
    <code>YourEvaluator</code>. The method takes as its input a board
    position <code>p</code> which specifies the positions of all the
    pieces on the board and which player's turn it is. The return value
    should be the higher the more likely the white player is to win.
  <li>
    You can upload the compiled <code>YourEvaluator</code> class
    (i.e., the <code>class</code> file) on
    the <a href="http://ejaasaar.users.cs.helsinki.fi/">tournament
    server</a>. (If you use multiple classes, you can also upload a JAR
    file.)
  <li>
    Once you have uploaded your bot, you can play it against the other
    bots. You gain points by winning and lose points by losing. The
    highest ranked bot will be declared the champion at the end of the
    course. (One week before the exam.)
</ol>

<% end %>

<% partial 'partials/material_heading' do %>
  Probabilistic Inference
<% end %>

<p>
  Let's now move ahead with the theme Reasoning under Uncertainty,
  and see how probability can be used for inference in various
  AI problems.
</p>

<% partial 'partials/material_sub_heading' do %>
  Bayes rule and probabilistic inference
<% end %>

<p>
  Bayes rule has a central role in statistical inference and machine
  learning. The basis for this is that it can be applied in scenarios
  where one of the random variables in the model corresponds to an
  unknown "state" which we are interested to learn. If the model
  includes other variables that correspond to "observations" that
  can be made and that are dependent on the unknown state, we
  can use the Bayes rule as follows
</p>
<pre>
  P(state | observation) = P(state) P(observation | state) / P(observation)
</pre>
<p>
  The left side of the equation is called the <strong>posterior
  probability</strong> (probability after the observation).
  The first factor on the right is called the <strong>prior
    probability</strong> (probability prior to the observation),
  the middle factor is called the <strong>likelihood</strong>
  (how likely the observation is when the state is given), and
  the last term on the right has many names, of which 
  <strong>the annoying denominator</strong> may be the most
  fitting. (Evidence and marginal likelihood are more
  common.)
</p>

<% partial 'partials/hint', locals: { name: 'On (not) calculating the annoying denominator' } do %>

<p>
  As you may have noticed, when completing last week's exercises,
  and as you will notice at the latest when completing this week's,
  calculating the denominator may indeed be annoying.
</p>

<p>
  A nice trick that can sometimes save a lot of effort is to
  calculate the <strong>ratio</strong> of posterior probabilities,
  instead of the probabilities themselves. For example, consider
  the following ratio
</p>
<pre>
     R = P(State=1 | obs) / P(State=2 | obs)
</pre>
<p>
  where <code>obs</code> is an abbreviation for observation. 
  Applying Bayes rule to both the numerator and the denominator
  in the ratio, we'll notice that the denominator <code>P(obs)</code>
  cancels:
</p>
<pre>
     P(State=1) P(obs | State=1) / P(obs)   P(State=1) P(obs | State=1)
R =  ------------------------------------ = ---------------------------
     P(State=2) P(obs | State=2) / P(obs)   P(State=2) P(obs | State=2)
</pre>
<p>
  In case the two states (1 and 2) are the only possibilities, we
  have <code>P(State=2|obs) = 1-P(State=1|obs)</code> (because one of
  the events must happen and both cannot occur at the same time).
  In this case, after calculating the the above ratio, <code>R</code>,
  it can be mapped back into the posterior probability of state 1 by
</p>
<pre>
  P(State=1 | obs) = R / (1+R)
</pre>
<p>
  You can check that this is true by assigning and using the
  fact <code>P(State=2|obs) = 1-P(State=1|obs)</code>.
</p>

<% end %>

<% partial 'partials/material_sub_heading' do %>
  Naive Bayes classification
<% end %>

<p>
  One of the most useful applications of the Bayes rule is the so
  called <strong>naive Bayes classifier</strong>. It is a machine
  learning technique that can be used to <strong> classify</strong>
  objects such as text documents into two or more classes. The
  classifier is trained by analysing a set of <strong>training
  data</strong>, for which the correct classes are given.
</p>

<p>
  The naive Bayes model is a probabilistic model that involves a class
  variable -- this corresponds to the state variable above -- and a
  number of feature variables. The assumption in the model is that the
  feature variables are <strong> conditionally independent</strong>
  given the class.  (We will not discuss the exact meaning of
  conditional independence on this course. You can find more about it
  from the literature. For our purposes, it is enough to be able to
  exploit conditional independence in building the classifier.)
</p>

<p>
  We will use a <strong>spam email filter</strong> as a running
  example for illustrating the idea of the naive Bayes
  classifier. Thus, the class variable indicates whether a message is
  spam (or "junk email") or whether it is a legitimate message (also
  called "ham").  The words in the message correspond to the feature
  variables, so that the number of feature variables in the model
  is determined by the length of the message.
</p>

<p>
  To define the naive Bayes model, we need to specify the distribution
  of each variable. For the class variable, this is the distribution
  of spam vs ham messages, which we can for simplicity assume to be
  1:1, i.e., <code>P(spam) = P(ham) = 0.5</code>.
</p>

<p>
  For the feature variables, we will define two distributions: one for
  the spam messages and another one for the ham messages. In each case,
  we will make the simplifying assumption that each of the words in the
  message is distributed according to the same distribution. And as we
  said above, we also assume that the words are independent of each 
  other given the spam/ham class.
</p>

<p>
  The word distributions for the two classes are best estimated from
  actual training data, i.e., a corpus of spam messages and a corpus
  of legitimate messages. The simplest way is to count how many times
  each word, <code>aardvark, aardwolf,
  ..., Zyzzogeton</code>, appears in the corpus and dividing
  the number by the total number of words in the corpus.
</p>

<p>
  To illustrate the idea, let's assume that we have at our disposal
  both a spam corpus and a ham corpus. You can easily obtain one
  by saving a batch of your emails in two files.
</p>

<p>
  Assume that we have calculated the number of occurrences of the
  following words in the two classes of messages:
</p>
<pre>
        word    spam     ham
 ----------- ------- -------
 million         156      98
 dollars          29     119
 adclick          51       0
 conferences       0      12
 ----------- ------- -------
       total   95791  306438
</pre>

<p>
  One problem with estimating the probabilities directly from the
  counts is that the zero counts lead to zero estimates. This can
  be quite harmful for the performance of the classifier -- it
  easily leads to situations where the ratio of the posterior
  probabilities obtained as 0/0. To rectify the problem, the
  simplest solution is to use a small lower bound for all
  probability estimates. The value 0.000001, for instance, does
  the job.
</p>

<p>
  We can now estimate that the probability that a word in a spam
  message is <code>million</code>, for example, is about 156/95791
  &asymp; 0.0016285. In other words, on the average, roughly every
  614th word in a spam message is <code>million</code>. Likewise, we
  get the estimate 0.0003198 for the probability that a word in a ham
  message is <code>million</code>. Both of these probability estimates
  are small but more importantly, the former is higher than the
  latter. This turns out to be a sign that the word in question hints
  towards the message being spam -- which sounds logical. Words for
  which the ratio of the probabilities is the other way around, hint
  towards the message being ham.
</p>

<p>
  The following exercise will demonstrate the use of the naive Bayes
  model.
</p>

<% partial 'partials/exercise', locals: { name: 'Naive Bayes and Spam (Pencil-and-paper) (2p)' } do %>

<p>
  Consider the word counts in the above table.
</p>
<ol>
  <li>
    Estimate the remaining word probabilities for both classes.
  <li>
    Use the obtained estimates to calculate the probability
    <code>P(Word &ne; million)</code>, i.e., the probability that
    a single word in a message is <i>not</i> <code>million</code> when
    the class of the message is unknown. <i>Hint:</i> You should recall
    the rule for calculating the <b>marginal probability</b>
    (see, for example, the references given under Section 3.1
    Probability Fundamentals).
  <li>
    Calculate <code>P(spam | million)</code>, i.e., the probability
    that the message is spam given that its first word (or in fact,
    any particular word) is <code>million</code>.
  <li>
    Calculate <code>P(spam | million, dollars, adclick, conferences)</code>,
    i.e., the probability that the message is spam when its first four
    words are as stated.
</ol>
<p>
  Use the prior probability <code>P(spam) = 0.5</code>.
</p>
<p>
  <i>Additional hints:</i> In item 2, remember Bayes. In item 3, remember
  to use a lower bound on the estimates. Also recall the trick about
  not calculating the annoying denominator.
</p>

<% end %>
