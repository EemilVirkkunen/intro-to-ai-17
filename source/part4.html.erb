---
  title: Part 4
  exercise_page: true
  quiz_page: false
  published: true
---




<% partial 'partials/exercise', locals: { name: 'Bayesian networks (pencil-and-paper) (1p)' } do %>

<p>
  In Part 3, we briefly discussed how Bayesian networks lead to compact
  representation of probabilistic models.
  <ol>
    <li>
      Consider a model with N=4 variables, each of which has k possible
      values (0,1,...,k&ndash;1). How many different elementary events are there
      in the domain? One less is the number of probabilities that we'd have
      to determine in order to fully specify the model. (For example, in the
      Car model of Part 3, with N=6 and k=2, this number was 63.)
    <li> 
      How many parameters would we have to define to specify the model using
      an "empty" Bayesian network, i.e, a network with no edges at all?
    <li>
      What is the maximum number of parameters we'd have to define in a
      Bayesian network in this domain (N variables with k values each)?
      Remember that the graph has to be a DAG, i.e., no directed cycles are
      allowed.
    <li>
      <i>Optional:</i> Generalize to arbitrary values of N. It may be helpful to
      recall the sum of a geometric series 1+k+k<sup>2</sup>+...+k<sup>N&ndash;1</sup> =
      (k<sup>N</sup>&ndash;1)/(k&ndash;1), where k&ne;1.
  </ol>
</p>

<% end %>

<% partial 'partials/exercise', locals: { name: 'Bayesian networks: Car (programming) (1p)' } do %>

<p>
  Implement an algorithm for generating data from the Car network
  discussed in Part 3.  You should generate n tuples, each of which is
  a six element list of bits (or Boolean values). You should start by
  choosing the value of B so that it takes value 1
  (or <code>True</code>) with probability 0.9. After choosing the
  value of B, choose the value of R. If B=0, then R=0 with probability
  1. If B=1, then R=0 with probability 0.1. Continue this way,
  choosing the probabilities from the CPTs, until you have generated a
  complete tuple.
</p>

<p>
  Generate a sample of n=100 000 tuples. Use it to approximate the following
  conditional probabilities:
  <ol>
    <li>
      P(B | R,G,&not;S)
    <li>
      P(S | R,I,G)
    <li>
      P(S | &not;R,I,G)
  </ol>
  <strong>
    and (don't forget this part):
  </strong>
  <ol start=4><br>
    <li>
      Give an interpretation to the above probabilities. Are they in line with
      your intuition? In other words, do they make sense?
  </ol>
</p>

<% end %>

<% partial 'partials/exercise', locals: { name: 'Bayesian networks: Earthquake (programming) (1p)' } do %>

<p>
  Consider the following scenario. You live in an area where
  earthquakes happen on the average every 111th day. Another annoying
  thing is that a burglar breaks into your home on the average once a
  year. Both events occur independently of each other and uniformly
  throughout the year (so any day is like another).
</p>

<p>
  In case of an earthquake, your home alarm goes off with probability 0.81.
  If your home is broken into, the alarm goes off with probability 0.92.
  If these both should occur at the same time, the alarm goes off with
  probability 0.97. If nothing special is going on, there's a false alarm
  with probability 0.0095.
</p>

<p>
  Now modify the algorithm from the previous exercise so that you can generate
  data from variables [E]arthquake, [B]urglar, [A]larm. Generate a sample of 
  n=100 000 tuples.
  <ol>
    <li>
      Among the tuples with A=1, what is the fraction where B=1?
    <li>
      Among the tuples with A=1 and E=1, what is the fraction where B=1?
  </ol>
  <strong>
    and (don't forget these):
  </strong>
  <ol start=3><br>
    <li>
      Give an interpretation to your answers for items 1 & 2. What
      probabilities do they approximate?
    <li>
      Are they in line with your intuition? In other words, do they
      make sense? In particular: which of the fractions is bigger and
      why?
    <li>
      Repeat the experiment a couple of times to get a feeling of how much
      the results change just by chance. (This is the nature of the Monte
      Carlo approximation.) Experiment with different values of n. How does
      it affect the variability?
  </ol>
</p>

<% end %>


You can find the lecture slides for this part on the <a href="https://courses.helsinki.fi/DATA15001/119123276">course homepage</a>
under Materials.


<% partial 'partials/material_heading' do %>
  Machine Learning
<% end %>

<p>
</p>

<% partial 'partials/hint', locals: { name: 'Learning objectives of Part 4' } do %>

<table class="table">
  <tr>
    <td>
      Theme
    </td>
    <td>
      Objectives (after the course, you ...)
    </td>
  </tr>
    <td>
      Machine learning
    </td>
    <td>
      <ul>
	<li>can distinguish between unsupervised and supervised machine learning scenarios
	<li>can implement at least two supervised classification methods (e.g., naive Bayes, nearest neighbour classifier)
	<li>know the main types of neural networks (feed-forward, recurrent, self-organizing map) and their main principles
	<li>can implement the perceptron algorithm in a simple binary classification problem
      </ul>
    </td>
  </tr>
</table>

<% end %>

<% partial 'partials/material_sub_heading' do %>
  Kinds of Machine Learning
<% end %>

<p>
  <ol>
    <li>Supervised learning
    <li>Unsupervised learning
    <li>Reinforcement learning
  </ol>
</p>

<% partial 'partials/material_sub_heading' do %>
  k-Nearest Neighbor (k-NN) Classifier
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Naive Bayes Classifier
<% end %>


<% partial 'partials/material_heading' do %>
  Neural Networks
<% end %>

<% partial 'partials/material_sub_heading' do %>
  What is Special About Neural Networks?
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Weights and Activation
<% end %>

<% partial 'partials/material_sub_heading' do %>
  The Perceptron Algorithm
<% end %>

<% partial 'partials/exercise', locals: { name: 'Perceptron (programming) (2p)' } do %>

<p>
  Use the Perceptron template on TMC. (You can also implement your solution
  in other languages than Java. You'll find the necessary data files in the
  TMC template.)
</p>

<p>
  The file <code>mnist-x.data</code> contains 6000 images from the popular
  <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a>, each
  of which is given on a single line in the file. Each image consists
  of 28 &times; 28 pixels listed row-by-row, so each line in the file
  contains 784 values.  Each pixel is either black (&ndash;1) or white
  (1). The file
  <code>mnist-y.data</code> contains the correct class value (0-9) of each
  of the 6000 images.
</p>

<p>
  Use the first 5000 images as training data and the last 1000 as test data.
  <ol>
    <li>
      Run the Java template to make sure that the file <code>test100.bmp</code>
      is created. It should include the first 100 images sorted according to
      the correct class label. This verifies that reading the file was
      successful.
    <li>
      Modify the <code>train()</code> method in
      class <code>Perceptron</code> so that it learns to distinguish
      number 3 from number 5. (Notice that the
      variable <code>targetChar</code> can be set to be one of these
      classes while <code>oppositeChar</code> should be the
      other. This will set the class label as 1 and &ndash;1 as is
      required in the Perceptron algorithm. Images representing other
      numbers are ignored.) Try to get a classification error around
      5&ndash;15 %.
    <li>
      Try other pairs of number than 3 vs 5. Which numbers are easiest to
      classify and which are the hardest?
  </ol>
<% end %>

<% partial 'partials/exercise', locals: { name: 'Nearest Neighbor (programming) (1p)' } do %>

<p>
  Now implement a nearest neighbor classifier for the MNIST data used in
  the Perceptron exercise.
</p>

<p>
  Given a test example, X<sup>(test)</sup>, the classifier will find the nearest
  training data point, X<sup>(NN)</sup>, and return its class label
  Y<sup>(NN)</sup>. Notice that unlike the Perceptron, or most other classifiers,
  the nearest neighbor classifier doesn't really involve a training stage.
  All the action happens in the classification (testing) stage.
</p>

<p>
  Test your classifier using the same train/test split (5000/1000) as before.
  You can use the same pairs of numbers (3 vs 5), or even try classifying all
  the classes at the same time because the NN classifier is not restricted
  to binary classification. (Note that in multiclass classification, the
  expected accuracy tends to be lower than in binary classification simply
  because the problem is harder.)
</p>

<% end %>

<% partial 'partials/material_sub_heading' do %>
  Recurrent Neural Networks
<% end %>

<% partial 'partials/material_sub_heading' do %>
  The Self-Organizing Map (SOM)
<% end %>

