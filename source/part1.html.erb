---
  title: Part 1
  exercise_page: true
  quiz_page: true
  published: true
---


<% partial 'partials/material_heading' do %>
  What is AI?
<% end %>

<p>
  In this first part, we will discuss what we mean when we talk about AI.
  It turns out that there is no exact definition at all, but that the
  field is rather being redefined at all times.
</p>

<p>
  We will also briefly discuss the philosophical aspects of AI: whether
  intelligent behavior implies or requires the existence of a ''mind'', and
  in what extent is consciousness replicable as a computational process.
  However, this course is first and foremost focused on building 
  practically useful AI tools, and we will quickly push considerations 
  about consciousness aside as they tend to be only in the way when 
  designing working solutions to real problems.
</p>

<p>
  The first technical topic, also covered in Part 1, is 
  problem-solving by search, and games.
</p>
  
<% partial 'partials/hint', locals: { name: 'Learning objectives of Part 1' } do %>

<table class="table">
  <tr>
    <th>
      Theme
    </th>
    <th>
      Objectives (after the course, you ...)
    </th>
  </tr>
  <tr>
    <td>
      Philosophy and history of AI
    </td>
    <td>
      <ul>
	<li>can express the basic philosophical problems related to AI
(the difficulty in defining AI and consciousness, acting vs thinking,
Turing test) 
	<li>can distinguish between realistic and unrealistic AI in
science-fiction 
	<li>can describe the contrast between "Good Old Fashioned
AI" (GOFAI) and modern AI approaches 
	<li>know the main-stream developments in the history of AI
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      Games and search
    </td>
    <td>
      <ul>
	<li>can formulate a problem as a graph and apply search algorithms to solve it
	<li>can explain and implement A* search
	<li>can formulate a simple game (such as tic-tac-toe) as a game tree
	<li>can explain and implement the minimax algorithm and depth-limited alpha-beta pruning
	<li>can design a reasonable heuristic evaluation function in a game (e.g., chess)
      </ul>
    </td>
  </tr>
</table>

<% end %>


<% partial 'partials/material_sub_heading' do %>
  A (Very) Brief History of AI
<% end %>

<p>
  Artificial Intelligence (AI) is a subdiscipline of Computer
  Science. Indeed, it is arguably as old as Computer Science itself:
  <a href="https://en.wikipedia.org/wiki/Alan_Turing">Alan Turing</a>
  (1912-1954) proposed the Turing machine -- the formal model
  underlying the theory of computation -- as a model with equivalent
  capacity to carry out calculations as a human being (ignoring
  resource constraints on either side).
</p>

<p>
  The term AI was proposed by John McCarthy (1927-2011) -- often
  referred to as the Father of AI -- as the topic of a summer seminar,
  known as <a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">
  Dartmouth conference</a> held in 1956 at Dartmouth College.
</p>

<p>
  As computers developed to the level where it was feasible to
  experiment with practical AI algorithms in the 1940s and 1950s, the most
  distinctive AI problems were games. Games provided a convenient
  restricted domain that could be formalized easily. Board games such
  as checkers, chess, and (recently quite prominently) Go, have
  inspired countless researchers,  and continue to do so.
</p>

<p>
  Closely related to games, search and planning algorithms were an
  area where AI lead to great advances in the 1960s: in a little
  while, we will be able to admire the beauty of, for example,
  the <a href="https://en.wikipedia.org/wiki/A*_search_algorithm">A*
  search algorithm</a> and
  <a href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning">alpha-beta
  pruning</a>, and apply them to solve AI problems.</p>

<% partial 'partials/hint', locals: { name: 'The Elusive Definition of AI' } do %>

<p>
  There's an old (geeky) joke that AI is defined as ''cool things that
  computer can't do.'' The joke is that under this definition, AI can
  never make any progress: as soon as we find a way to do something cool
  with a computer, it stops being an AI problem.
</p>

<p>
  However, there is an element of truth in the definition in the sense
  that fifty years ago, for instance, search and planning algorithms
  were considered to belong to the domain of AI. Nowadays algorithms
  such as breadth-first and depth-first search, and A*, are thought 
  (and taught) as belonging to Data Structures and Algorithms.
</p>

<% end %>

<p>
  The history of AI, just like many other fields of science, has
  witnessed the coming and going (and coming back and going again,
  etc.) of various different paradigms. Typically, a particular
  paradigm is adopted by most of the research community and
  ultra-optimistic estimates of progress in the near-future are
  provided. All such scenarios so far have ended up running into
  unsurmountable, unexpected problems and the interest has died
  out. For example, in the 1960s <b>artificial neural networks</b>
  were widely believed to solve all AI problems by imitating the
  learning mechanisms in the nature (such as the human central nervous
  system and the brain, in particular). However, certain negative
  results about the expressibility of certain neural computation
  models quickly lead to pessimism and an <b>AI winter</b> followed.
</p>

<p> 
  The 1980s brought a new wave of AI methods based on logic-based
  methods. So called <b>expert systems</b>, manipulating knowledge
  elicited from domain experts, such as medical doctors, showed
  great promise by solving nicely contained, well-defined 
  ''toy problems'', but turned out to fail every time when they
  were deployed in more complex, real-world problems. The second
  (or the third, depending on the counting) AI winter lasted from the
  late 1980s until the mid-1990s.
</p>
 
<p>
  Currently, since the turn of the millennium, AI has been on the rise
  again. In the late 1990s, the ''classical'' or <b>''Good
  Old-Fashioned AI'' (GOFAI)</b> that addressed crisp, clearly
  defined, and isolated problems begun to be replaced by so called
  <b>''modern AI''</b> (in lack of a better name). Modern AI
  introduced methods that were able to handle uncertain and imprecise
  information, most notably by probabilistic methods, and which had
  the great advantage that it was designed to work in the real world.
  The rise of modern AI has continued until present day, further
  boosted by the come-back of neural networks under the label <b>Deep
  Learning</b>.
</p>

<p>
  Whether the history will repeat itself, and the current boom will be
  once again followed by an AI winter, is a matter that only time can
  tell. Even if it does, the significance of AI in the society is
  going to stay. Today, we live our life surrounded by AI, most of the
  time happily unaware of it: the music that we listen, the products
  that we buy online, the movies and series that we watch, our routes
  of transportation, and the information that we have available, are
  all influenced more and more by AI.
</p>

<p>
  No wonder that you have decided to learn more about AI!
</p>

<p> Being able to apply AI methods and thus to be part of the progress
  of AI is a great way to change the world for the better. And even if
  you wouldn't aspire to become an AI researcher or developer, it is
  almost your duty as a citizen to understand at least the
  fundamentals of AI so that you can better use it: be aware of its
  limitations and enjoy all the goodies it can provide.
</p>

<% partial 'partials/material_sub_heading' do %>
  On the Philosophy of AI
<% end %>



<p>
  The best known contribution to AI by Turing is his <i>imitation
  game</i>, which later became known as the <a href="https://en.wikipedia.org/wiki/Turing_test">Turing test</a>.  In the
  test, a human interrogator interacts with two players, A and B, by
  exchanging written messages (in a ''chat''). If the interrogator
  cannot determine which player, A or B, is a computer and which is a
  human, the computer is said to pass the test.  The argument is that
  if a computer is indistinguishable from a human in a general
  natural language conversation, then it must have reached human-level
  intelligence.
</p>

<p>
  Turing's argument that whether a being is intelligent or not can be
  decided based on the behavior it exhibits has been challenged by
  some. The best known counter-argument is John Searle's
  <a href="http://www.iep.utm.edu/chineser/">Chinese Room</a> thought
  experiment. Searle descibes an experiment where a person who doesn't
  know Chinese is locked in a room. Outside the room is a person who
  can slip notes written in Chinese inside the room through a mail
  slot. The person inside the room is given a large manual where she
  can find detailed instructions for responding to the notes she
  receives from the outside.
</p>

<p>
  Searle argued that that even if the person outside the room gets the
  impression that he is in a conversation with another
  Chinese-speaking person, the person inside the room does <i>not
  understand</i> Chinese. Likewise, his argument continues, even if a
  machine behaves in an intelligent manner, for example, by
  passing the Turing test, it doesn't follow that it <i>is</i>
  intelligent or that it has a ''mind'' in the way that a human
  has. The word ''intelligent'' can also be replaced by the word
  ''self-conscious'' and a similar argument can be made.
</p>

<p>
  The definition of intelligence, natural or artificial, and
  consciousness appears to be extremely evasive and leads to
  apparently never-ending discourse. In an intellectual company, with
  plenty of good Burgundy (Bordeaux will also do), this discussion can
  be quite enjoyable. However, as John
  McCarthy <a href="http://www-formal.stanford.edu/jmc/aiphil/node2.html#SECTION00020000000000000000">pointed
  out</a>, the philosophy of AI is ''unlikely to have any more effect
  on the practice of AI research than philosophy of science generally
  has on the practice of science.''
</p>

<% partial 'partials/exercise', locals: { name: 'What is AI, really? (1p)' } do %>

<p>
  Your first exercise will be to take a look into current AI research.
  Find an AI-related scientific article from recent years. Pick one
  that you can understand, by and large: try to see what the problem
  statement, methodology, and conclusions are, roughly.
</p>

<p>
  Good places to start your search are, e.g., the proceedings of
  <a href="http://www.aaai.org">AAAI</a>,
  <a href="http://www.ijcai.org">IJCAI</a>, and 
  <a href="https://www.eurai.org/activities/ECAI_conferences">ECAI</a>
  conferences or magazine-style publications that may be somewhat less
  technical and intended for broader audiences, such
  as <a href="http://ai-magazine.com/">AI Magazine</a>. However,
  please try to avoid articles that are overly polemic and
  superficial -- the idea is to take a look at <i>academic</i> AI,
  and ignore the BS on my Facebook feed...
</p>

<p>
  Read the article through and answer the following questions:
  <ol>
    <li> What is the research problem?
    <li> Is the topic related to the topics of this course?
    <li> Generally speaking, what impression does the article give
      about modern AI research? Reflect on the history and philosophy of
      AI discussed above.
    <li> What studies would be needed to undertand the article in detail?
    <li> Bonus question: Considering the article you chose, how relevant
      is the ''Terminator'' scenario where AI becomes self-conscious and
      turns against the humankind?
  </ol>
</p>

<% end %>

<% partial 'partials/hint', locals: { name: 'How do I return my solutions?' } do %>

<p>
  Solving the above exercise gives you one point (1p). Some exercises such
  as Ex1.4 below require a bit more effort, and they may give you
  two point (2p). This is indicated in the exercise heading as above.
</p>

<p>
  Solutions to ''pen-and-paper'' exercises such as this one 
  are returned at the exercise sessions where you should make sure
  to mark completed exercises on the sheet that is circulated in
  the beginning.
</p>

<p>
  For programming exercises, you will be able to use the TMC system
  which helps you see whether the solution is correct. However,
  even the TMC exercises are marked at the exercise session.
  In other words, <b>it is not enough to upload the programming
    exercises on TMC to get the points</b>.
</p>

<% end %>



<p>
  We will now put our wine glasses aside, roll our sleeves, and turn
  our minds toward more practical considerations.
  Let's jump to our first technical topic: 
  search and problem-solving.
</p>

<% partial 'partials/material_heading' do %>
  Search and Problem-Solving
<% end %>

<p>
  Many problems can be phrased as search problems. Formulating the
  search space and choosing an appropriate search algorithm often
  requires careful thinking and is an important skill for an AI
  developer.
</p>

<p>
  Basic tree and network traversal algorithms belong to the
  course prerequisites, and you should already be familiar with 
  breadth-first, depth-first, and best-first search (including its
  special case, the A* algorithm). If you forgot the details right
  after taking the exam, no need to worry: we will revisit them
  below.
</p>

<% partial 'partials/material_sub_heading' do %>
  Breadth-First and Depth-First Search
<% end %>

<p>
  To set the scene for discussing more advanced search algorithms,
  such as A*, we begin by defining a generic templace for search
  algorithms.
</p>

<% partial 'partials/code_highlight' do %>
1:  search(start_node):
2:     node_list = list()                # empty list (queue/stack/...)
3:     visited = set()                   # empty set
4:     add start_node to node_list
5:     while list is not empty:
6:        node = node_list.first()       # pick the next node to visit
7:        remove node from node_list
8:        if node not in visited:
9:           visited.add(node)
10:          if goal_node(node):
11:             return node              # goal found
12:          add node.neighbors() to node_list
13:       end if
14:    end while
15:    return None                       # no goal found
<% end %>

<p>
  In the above pseudo-code, <code>node_list</code> holds the nodes to
  be visited. The order in which nodes are taken from the list
  by <code>node_list.first()</code> determines the behavior of the
  search: a queue (first-in, first-out) results in <b>breadth-first
  search (BFS)</b> and a stack (last-in, first-out) results
  in <b>depth-first search (DFS)</b>.
</p>

<p>
  In case of BFS, the operation of adding a node to the list (queue)
  is <i>enqueue</i> and the operation of removing the node that was
  added first is <i>dequeue</i>.
</p>

<p>
  In the case of DFS, the operation of adding a node to the list (stack)
  is <i>push</i>, and the operation of removing the node that was
  added last is <i>pop</i>.
</p>

<p>
  The test <code>goal_node</code> tests whether the goal or target node
  of the search is found. Sometimes the problem is simply to traverse the
  network (or tree) completely in a particular order, and there is no
  goal node. In that case, <code>goal_node</code> simply always returns
  <code>False</code>.
</p>

<% partial 'partials/hint', locals: { name: 'But this isn&rsquo;t how Granma taught it to me!' } do %>
<p>
  You may have seen different versions of search algorithms and wonder why
  this isn't exactly like them. In particular, many students have been
  taught the recursive version of DFS, which indeed is very simple
  and elegant.
</p>

<p>
  You need not worry about the difference too much. Here we simply wanted
  to use the same template for all search methods. The behavior is 
  always the same: for example, the recursive version of DFS actually
  uses a stack to store the state of the search and pops the next
  state from the stack just like our non-recursive version above.
</p>
<% end %>

<p>
  It is quite straighforward to see that BFS will always return the
  path with the fewest transitions to a goal node: if node A is nearer
  to the starting node than node B, the search is expanded to node A
  earlier than to B. You can think of the BFS search as
  a <i>frontier</i> of nodes that gradually progresses outwards from 
  the starting node, so that all nodes at a certain number of 
  steps away are expanded before moving one step ahead.
</p>

<p>
  DFS doesn't guarantee that the shortest path be found, but in some
  cases it doesn't matter. See the lecture slides for an example of
  solving Sudoku puzzles using DFS. Can you think of a reason by
  DFS is a better choice in that problem that BFS?
</p>

<p>
  Here's a simple exercise to make sure BFS and DFS are clear enough.
</p>

<% partial 'partials/exercise', locals: { name: 'Breadth-first and depth-first search (1p)' } do %>

<p>
  <img width=30% src="/img/exercises/ex1/Drawing.png" align=right>
  Consider the (cute) network on the right.
</p>

<p>
  <ol>
    <li> Simulate (on pen-and-paper) breadth-first search starting from
      node A when the goal node is H.
    <li> Do the same with depth-first search.
  </ol>
  In each case, present the contents of the node list (queue or stack)
  at each step of the search. To ensure that everyone gets the same
  result, let's agree that nodes are added to the list in alphabetical
  order.
</p>

<% end %>

<p>
  As we discussed above while drinking red wine, search algorithms don't
  necessarily feel like being very cool AI methods. However, as the
  next two exercises demonstrate, they can actually be used to solve
  tasks that -- most of us would admit -- require intelligence.
</p>

<% partial 'partials/exercise', locals: { name: 'Towers of Hanoi (1p)' } do %>

<p>
  Let's play. Solve the well-known
  puzzle <a href="https://www.britannica.com/topic/Tower-of-Hanoi">Towers
    of Hanoi</a>. The puzzle involves three pegs, and three discs: one
  large, one medium-sized, and one small.  (Actually, there can be any
  number of discs but for the pen-and-paper exercise, three is plenty.)
</p>

<p>In the initial state, all three discs are stacked in the first
  (leftmost) peg. The goal is to move the discs to the third peg.
  You can only move one disc at a time, and it is not allowed to
  put a larger disc on top of a smaller disc.</p>

<p>This pretty picture shows the initial state and the goal state:
<pre>
initial  |     |     |          goal    |     |     |          
state:  ---    |     |          state:  |     |    ---  
       -----   |     |                  |     |   -----     
      =====================         ====================
</pre>
</p>

<p>
  <ol>
    <li> Draw a network diagram where the nodes are all the states
      that can be achieved from the initial state, and the edges
      represent allowed transitions (moves) between them.
    <li> Simulate breadth-first search in the state space.
      <b>Note:</b>You don't have to explicitly specify the
      contents of the queue at each step. It is enough to provide
      the traversal order.
    <li> Do the same with depth-first search.
    <li> Compare the search methods on two accounts: <i>a)</i>
      what is the length of the path that each algorithm finds,
      <i>b)</i> what is the number of states visited during the
      search. <b>Note:</b> It is important to note that these are
      two different things (the length of the path, and the number
      of visited states.)
    <li> Does the result depend on the order in which the neighbors
      of each node are added into the list?
  </ol>
</p>

<p>A bonus exercise: Try to see the symmetry in the state diagram,
  and generalize to <i>n > 3</i> discs.
</p>

<% end %>

<% partial 'partials/exercise', locals: { name: 'Travel Planner (2p)' } do %>

<p>
  Now it's time for this week's highlight: implementing a real AI
  application. It will require some effort, and programming can often
  be slow and frustrating, but stay focused, don't hesitate to ask for
  help, and you'll be ok. Next week, we'll continue working on the
  same application, so your hard work now will make your life easier
  next week.
</p>

<p>
  The task is to read Helsinki tram network -- outdated, we're afraid
  so it's not going to be super useful -- data from a file that we give.
  Implement a program that takes as input the starting point A and
  the destination B, and finds the route from A to B with the
  <i>fewest stops</i> between them. It is quite straightforward to 
  show that such a route can be found by BFS.
</p>

<p>
  We provide a Java template that includes a <code>Stop</code> class
  which can retrieve the neighboring stops. These are the valid
  transitions in the state space.
</p>

<p>
  You can also start from scratch and implement your solution in
  your favorite programming language. In that case, simply take the
  <code>network.json</code> file, which is pretty self-explanatory.
  A hint to python programmers: <code>import json</code>.
</p>

<p>
  If using the Java template (others may find these instructions
  useful too):
  <ol>
    <li>
      Download and open the Maven project in a suitable development
      environment (e.g., Netbeans).
    <li>
      Implement the search algorithm in
      class <code>TravelPlanner</code>.  
    <li>
      In order to be able to
      extract the resulting route after the search ends, construct a
      backward-linked list of <code>Stop</code> objects as the stops
      are added into the queue, each of which has a pointer to the
      previous stop from which the search arrived at the stop in
      question. This way, once you arrive at the destination, you can
      start backtracking along the shortest path until the beginning.
    <li>
      Test your solution on TMC to see that your TravelPlanner works
      as it should and doesn't send you on a detour.
  </ol>
</p>

<p>
  If you don't use TMC, you can test by setting the starting stop as
  <code>1250429(Metsolantie)</code> and the destination as
  <code>1121480(Urheilutalo)</code>. The path (listed backwards) with
  the fewest stops is as follows:
<pre>
1121480(Urheilutalo)[DESTINATION] -> 1121438(Brahenkatu) -> 1220414(Roineentie)
-> 1220416(Hattulantie) -> 1220418(Rautalammintie) -> 1220420(Mäkelänrinne)
-> 1220426(Uintikeskus) -> 1173416(Pyöräilystadion) -> 1173423(Koskelantie)
-> 1250425(Kimmontie) -> 1250427(Käpylänaukio) ->1250429(Metsolantie)[START]
</pre>
</p>

<% end %>

<p>
  Alright. So far, we've refreshed BFS and DFS in our memory and
  applied them to solve some pretty cool applications. To go to the
  next level, we'll bring out the big guns, and talk about best-first
  search (which is <b>not</b> abbreviated in order to avoid confusing
  it with breadth-first search) and the A* algorithm.
</p>

<p>
  
</p>

<% partial 'partials/material_sub_heading' do %>
  Informed Search and A*
<% end %>

<p>
  Often, different transitions in the state space are associated with
  different costs.  For example, doing a task could take
  any time between a few seconds and several hours. Or the distance
  between any two tram stops could be between a hundred meters and half
  a kilometer.  Thus, just counting the <i>number</i> of transitions
  is not enough.
</p>

<p>
  To be able to take into account different costs, we can apply
  <b>best-first search</b>, where the node list is ordered by a given
  criterion. For instance, we can choose to always prefer to expand a
  path with the minimal incurred total cost counting from the starting
  node. This is known as <b>Dijkstra's algorithm</b>. In the special
  case where the cost of all transitions is constant, Dijkstra's
  algorithm is equivalent to BFS.
</p>

<p>
  The generic search algorithm template above still applies, but in
  best-first search, the data structure that holds the nodes on the
  node list is a <b>priority queue</b>. When adding nodes to the
  priority queue on line 14, they are given a cost or a value that is
  then used to order the nodes in the queue.  (Depending on the
  application and whether the aim is to minimize or maximize the
  value, the queue can be a min-priority queue or a max-priority
  queue.)
</p>

<% partial 'partials/hint', locals: { name: 'Alice, Where Art Thou Going?' } do %>

<p>
  If you play around with 
  the <a href="http://qiao.github.io/PathFinding.js/visual/">PathFinding
    applet</a> for a while, using BFS or Dijkstra's algorithm, you
  will quickly notice a problem. The search spreads out to all 
  directions symmetrically without any preference towards the 
  goal.
</p>

<p>
  This is understandable since the choice of the next node to expand
  has nothing to do with the goal. However, if we have some
  way of measuring, even approximately, which nodes are nearer
  to the goal, we can use it to guide the search and
  save a lot of effort by never having to explore unpromising
  paths. This is the idea behind <b>informed search</b>.
</p>

<% end %>

<p>
  Informed search relies on having access to a
  <b>heuristic</b> that associates with each node an estimate of the
  remaining cost from the node to the goal. This can be, for example,
  the distance between the node and the goal measured as the crow
  flies (i.e., Euclidean or geodesic distance -- or in plain words, a
  straight-line distance).
</p>

<p>
  Using the heuristic as the criterion for ordering the nodes in
  the (min-)priority queue will always expand nodes that appear to
  be nearer to the goal according to the heuristic. However, this
  may lead the search astray because the incurred cost of the path
  is not taken into account. A balanced search that takes both the
  incurred cost as well as the estimated remaining cost into account
  is obtained by ordering the (min-)priority queue by
  <pre>
           f(node, cost) = cost + h(node),
  </pre>
  where <code>cost</code> is the value associated with the node when
  it is added to the priority queue, and <code>h(node)</code> is the
  heuristic value, i.e., an estimate of the remaining cost
  from <code>node</code> to the goal. This is the <b>A* search</b>.
  If you try it on
  the <a href="http://qiao.github.io/PathFinding.js/visual/">PathFinding
  applet</a>, you will immediately see that it wipes the floor with
  other, uninformed search methods.
</p>  


<% partial 'partials/material_heading' do %>
  Games
<% end %>

<p>
  Maxine and Minnie are true game enthusiasts. They just love games.
  Especially two-person, perfect information games such as tic-tac-toe
  or chess.
</p>

<p>
  One day they were playing tic-tac-toe. Maxine, or Max as her
  friends call her, was playing with X. Minnie, or Min as her
  friends call her, had the Os. The situation was
<pre>
       O| |O
       -+-+-
       X| |
       -+-+-
       X|O|
</pre>
  Max was looking at the board and contemplating her next move, as it
  was her turn, when she suddenly buried her face in her hands in
  despair, looking quite like Garry Kasparov playing Deep Blue in
  1997.
</p>

<p>
  Yes, Min was close to getting three Os on the top row, but Max could
  easily put a stop to that plan. So why was Max so pessimistic?
</p>

<% partial 'partials/material_sub_heading' do %>
  Game Trees 
<% end %>

<p>
  To analyse games and optimal strategies, we will introduce the
  concept of a <b>game tree</b>. The game tree is similar to a search
  tree, such as the one in the Sudoku example discussed at the
  lecture.  (Remember that you should also study the lecture slides in
  addition to this material. Some material may be discussed in one but
  not the other.) The different states of the game are represented by
  nodes in the game tree. The "children" of each node N are the
  possible states that can be achieved from the state corresponding to
  N. In board games, the state of the game is defined by the board
  position and whose turn it is.
</p>

<p>
  Consider, for example, the following game tree which begins
  not at the root but in the middle of the game (because otherwise,
  the tree would be way too big to display).
  <br>
  <img width=80% src="/img/diagrams/tictactoe-tree.png">
  <br>
  The game continues at the board position shown in the root node,
  numbered as (1) at the top, with Min's turn to place O at any of the
  three vacant cells. Nodes (2)--(4) show the board positions
  resulting from each of the three choices respectively. In the next
  step, each node has two possible choices for Max to play X each,
  and so the tree branches again.
</p>

<p>
  The game ends when either player gets a row of three, or when there
  are no more vacant cells. When starting from the above starting
  position, the game always ends in a row of three.
</p>

<p>
  Now consider nodes (5)--(10) on the second layer from the bottom.
  In nodes (7) and (9), the game is over, and Max wins with three X's
  in a row. In the remaining nodes, (5), (6), (8), and (10), the game
  is also practically over, since Min only needs to place her O
  in the only remaining cell to win. We can thus decide that the
  end result, or the <b>value</b> of the game in each of the nodes
  on the second level from the bottom is determined. For the
  nodes that end in Max's victory, we'll say that the value 
  equals +1, and for the nodes that end in Min's victory, we'll say
  that the value is -1.
</p>

<p>
  More interestingly, let's now consider the next level of nodes
  towards the root, nodes (2)--(4). Since we decided that both of the
  children of (2), i.e., nodes (5) and (6), lead to Min's victory, we
  can without hesitation attach the value -1 to node (2) as well.  For
  node (3), the left child (7) leads to Max's victory, +1, but the
  right child (8) leads to Min winning, -1. However, it is Max's turn
  to play, and she will of course choose the left child without
  hesitation. Thus, every time we reach the state in node (3), Max
  wins. Thus we can attach the value +1 to node (3).
</p>

<p>
  The same holds for node (4): again, since Max can choose where to
  put her X, she can always ensure victory, and we attach the value
  +1 to node (4).
</p>

<p>
  So far, we have decided that the value of node (2) is -1, which
  means that if we end up in such a board position, Min can ensure
  winning, and that the reverse holds for nodes (3) and (4): their
  value is +1, which means that Max can be sure to win if she only
  plays her own turn wisely.
</p>

<p>
  Finally, we can deduce that since Min is an experienced player, she
  can reach the same conclusion, and thus she only has one real
  option: give Max an impish grin and play the O in the middle of the
  board.
</p>

<p>
  In the diagram below, we have included the value of each node as
  well as the optimal game play starting at Min's turn in the root
  node.
  <br>
  <img width=80% src="/img/diagrams/tictactoe-values.png">
</p>

<p>
  The value of the root node, which is said to be the <b>value of the
  game</b>, tells us who wins (and how much, if the outcome is not
  just plain win or lose): Max wins if the value of the game is +1,
  Min if the value is -1, and if the value is 0, then the game will
  end in a draw. This all is based on the assumption that both players
  choose what is best for them.
</p>

<p>
  The optimal play can also be deduced from the values of the nodes:
  at any <b>Min node</b>, i.e., node where it is Min's turn, the
  optimal choices are given by those children whose value is minimal,
  and conversely, at any <b>Max node</b>, where it is Max's turn, the
  optimal choices are given the the children whose value is maximal.
</p>

<% partial 'partials/material_sub_heading' do %>
  Minimax Algorithm
<% end %>

<p>
  We can exploit the above concept of the value of the game to obtain
  an algorithm with optimal game play in, theoretically speaking, any
  deterministic, two-person, perfect-information game. Given a state
  of the game, the algorithm simply computes the values of the
  children of the given state and chooses the one that has the maximum
  value if it is Max's turn, and the one that has the minimum value if
  it is Min's turn.
</p>

<p>
  The algorithm can be implemented using the neat recursive
  functions below for Max and Min nodes respectively. This is 
  known as the <b>Minimax algorithm</b> (see <a href="https://en.wikipedia.org/wiki/Minimax">Wikipedia: Minimax</a>).
</p>

<% partial 'partials/code_highlight' do %>
1:  max_value(node):
2:     if end_state(node): return value(node)
3:     v = -Inf
4:     for each child in node.children():
5:        v = max(v, min_value(child))
6:     return v  
<% end %>

<% partial 'partials/code_highlight' do %>
1:  min_value(node):
2:     if end_state(node): return value(node)
3:     v = +Inf
4:     for each child in node.children():
5:        v = min(v, max_value(child))
6:     return v  
<% end %>

<% partial 'partials/exercise', locals: { name: 'Minimax tic-tac-toe (1p)' } do %>

<p>
  Let's return to the tic-tac-toe game described in the beginning
  of this section. To narrow down the space of possible end-games
  to consider, we can observe that Max must clearly place an X on
  the top row to avoid imminent defeat:
  <pre>
       O|X|O
       -+-+-
       X| |
       -+-+-
       X|O|
  </pre>
  Now it's Min's turn to play an O. Evaluate the value of this state
  of the game as well as the other states in the game tree where the
  above position is the root, using the Minimax algorithm.
</p>

<% end %>

<% partial 'partials/hint', locals: { name: 'Sounds good, can I go home now?' } do %>

<p>
  As stated above, the Minimax algorithm can be used to implement
  optimal game play in any deterministic, two-player,
  perfect-information game. Such games include tic-tac-toe, connect
  four, chess, Go, etc. (Rock-paper-scissors is not in this class of
  games since it involves information hidden from the other player;
  nor are Monopoly or backgammon which are not deterministic.) So as
  far as this topic is concerned, is that all folks, can we go home
  now?
</p>

<p>
  The answer is that in theory, yes, but in practice, no. In many
  games, the game tree is simply way too big to traverse in full.  For
  example, in chess the average branching factor, i.e., the average
  number of children (available moves) per node is about 35.  That
  means that to explore all the possible scenarios up to only two
  moves ahead, we need to visit approximately 35 x 35 = 1225 nodes
  -- probably not your favorite pen-and-paper homework exercise...  A
  look-ahead of three moves requires visiting 42875 nodes; four moves
  1500625; and ten moves 2758547353515625 (that's about
  2.7 quadrillion) nodes.
</p>

<p>
  In Go, the average branching factor is estimated to be about 250.
  Go means no-go for Minimax.
</p>

<% end %>

<p>
  Next, we will learn a few more tricks that help us manage massive
  game trees, and that were crucial elements in IBM's Deep Blue
  computer defeating the chess world champion, Garry Kasparov, in 
  1997.
</p>

<% partial 'partials/material_sub_heading' do %>
  Depth-limited minimax and heuristic evalution criteria
<% end %>

<p>
  If we can afford to explore only a small part of the game tree,
  we need a way to stop the minimax recursion before reaching an
  end-node, i.e., a node where the game is over and the winner
  is known. This is achieved by using a <b>heuristic 
    evaluation function</b> that takes as input a board position,
  including the information about which player's turn is next,
  and returns a score that should be an estimate of the likely
  outcome of the game continuing from the given board position.
</p>

<p>
  Good heuristics for chess, for example, typically count the amount
  of material (pieces) weighted by their type: the queen is usually
  considered worth about two times as much as a rook, three times a
  knight or a bishop, and nine times as much as a pawn. The king is of
  course worth more than all other things combined since losing it
  amounts to losing the game. Further, occupying the strategically
  important positions, e.g., near the middle of the board, is
  considered an advantage.
</p>

<p>
  The minimax algorithm presented above requires minimal changes 
  to obtain a <b>depth-limited</b> version where the heuristic
  is returned at all nodes at a given depth limit.
</p>

<% partial 'partials/material_sub_heading' do %>
  Alpha-beta pruning
<% end %>

<p>
  Another breakthrough in game AI, proposed independently by several
  researchers including John McCarthy in and around 1960,
  is <a href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning">alpha-beta
  pruning</a>. For small game trees, it can be used independently of
  the heuristic evaluation method, and for large trees, the two can be
  combined into a powerful method that has dominated the area of game
  AI for decades.
</p>

<p>
  A good example of the idea behind alpha-beta-pruning can be seen
  in the tic-tac-toe game tree that we discussed above -- scroll
  up and let the image of the root node burn into your retina.
</p>

<p>
  Now simulate the Minimax algorithm at the stage where the value of
  the left child node, -1, has been computed and returned to the
  <code>min_value</code> function. The next step would be to
  call <code>max_value</code> to compute the value of the middle
  child. But hold on! If the left child guarantees victory for Minnie,
  what does it matter how the game ends if she chooses to play any
  other way? As soon as the algorithm finds a child node with the best
  possible outcome for the player whose turn it is, it can make a
  choice and avoid computing the values of all the other child nodes.
</p>

<p>
  To implement this in a similar fashion as the Minimax algorithm
  requires small changes in the <code>min_value</code> and 
  <code>max_value</code> functions. Understanding the connection
  between these changes and the principle illustrated by the above
  pruning example is not as easy as it may sound, so please pay
  close attention to this topic and work out the examples and
  exercises with care.
</p>

<% partial 'partials/code_highlight' do %>
1:  max_value(node, alpha, beta):
2:     if end_state(node): return value(node)
3:     v = -Inf
4:     for each child in node.children():
5:        v = max(v, min_value(child, alpha, beta))
6:        alpha = max(alpha, v)
7:        if alpha >= beta: return v
8:     return v  
<% end %>

<% partial 'partials/code_highlight' do %>
1:  min_value(node, alpha, beta):
2:     if end_state(node): return value(node)
3:     v = +Inf
4:     for each child in node.children():
5:        v = min(v, max_value(child, alpha, beta))
6:        beta = min(beta, v)
7:        if alpha >= beta: return v
8:     return v  
<% end %>

<p>
  An important thing to remember is that the <code>alpha</code> value
  is updated only at the Max nodes, and the <code>beta</code> value is
  updated only at the Min nodes. The updated values are passed as
  arguments down to the children, but not up to the calling parent
  node. (That is, the arguments are passed <i>as values</i>, not <i>as
  references</i> in programming lingo.)
</p>

<p>
  The interpretation <code>alpha</code> and <code>beta</code> is that
  they provide the interval of possible values of the game at the node
  that is being processed:
  <code>alpha</code> &le; value &le; <code>beta</code>. This interval
  is updated during the algorithm, and if at some point, the interval
  shrinks so that <code>alpha = beta</code>, we know the value and
  can return it to the parent node without processing any more
  child nodes. It can also happen that <code>alpha > beta</code>,
  which implies that the current node will never be visited in
  optimal game play, and its processing can likewise be aborted.
</p>

<p>
  When starting the recursion at the root node, we initialize their
  respective values as the minimum and maximum result of the game. For
  tic-tac-toe and chess, where the outcome is plain win/loss, this
  is <code>alpha = -1</code> and <code>beta = 1</code>. If the
  range of possible values is not specified in advance, we 
  initialize as <code>alpha = -&infin;</code> and <code>beta = 
    &infin;</code>.
</p>

<p>
  It is useful to work out a few examples to really understand the
  beauty of alpha-beta pruning. Here's another tic-tac-toe
  example.
  <br>
  <img width=80% src="/img/diagrams/tictactoe-alphabeta.png">
  <br>
</p>

<p>
  You should simulate the algorithm to see that the two branches
  that are grayed out  indeed get pruned -- therefore, it is
  actually a bit misleading to even show their minimax values
  since the algorithm never computes them.
</p>

<p>
  Remember that the Max nodes (such as the root node) only update
  the <code>alpha</code> value and pass it down to the next
  child node. Check that you reach a situation where 
  <code>alpha=0</code> and  <code>beta=-1</code> in a 
  Min node. Actually you should reach such a situation twice.
</p>

<p>
  Another good example (except for the choice of colors) can be
  found from Bruce Rosen's lecture notes for <i>Fundamentals of
  Artificial Intelligence - CS161</i> at
  UCLA: <a href="http://web.cs.ucla.edu/~rosen/161/notes/alphabeta.html">here</a>.  Note in particular how he emphasizes the fact that the
  tree is only expanded node by node as the algorithm runs, 
  instead of running the algorithm on a full tree as is often
  suggested by diagrams such as the ones we use in this material.
  Rosen's example also illustrates a scenario where the value of
  the game is not constrained to be -1,0, or +1, and the algorithm
  starts at the root with <code>alpha = -&infin;</code> and
  <code>beta = &infin;</code>..
</p>
