---
  title: Part 5
  exercise_page: true
  quiz_page: false
  published: true
---

<% partial 'partials/exercise', locals: { name: 'Machine Learning with Weka: MLP (1p)' } do %>

<p>
  Download and install the Weka system on your computer. We've
  prepared a set of instructions to get started and a minitutorial.
  <%= link_to 'Click here', 'part5_ex1.html'%>. You'll also need to
  download the Two Spirals dataset <a href="https://materiaalit.github.io/intro-to-ai-17/files/spirals.arff">here</a>.
</p>

<p>
  Use Weka to train a multilayer perceptron (MLP) to classify the
  Two Spirals data. The class labels are given in the <code>Class</code>
  column whereas the variables <code>A1</code> and <code>A2</code> are
  the x- and y-coordinates which are the input features.</p>

<p>
  Note that using the default settings gives poor results. When
  accuracy is measured using the <b>Percentage split % 66</b>
  setting, the accuracy is 52.9&nbsp;% (36 correct, 32 incorrect
  instances). Click <b>Visualize classification errors</b> and
  let the x- and y-axis be the input variables <code>A1</code> and
  <code>A2</code> respectively. You will be able to see that the
  decision boundary, which is completely bonkers.
</p>

<p>
  Adjust the MLP settings and experiment with different number of
  hidden layers (the parameter <code>hiddenLayers</code> shows the
  number of neurons on each hidden layer, separated by
  colons <code>:</code>), and the number of training iterations
  (<code>trainingTime</code>). You should get classification error
  less than 10&nbsp;%.
</p>

<% end %>

<% partial 'partials/exercise', locals: { name: 'Machine Learning with Weka: more classifiers (1p)' } do %>

<p>
  <ol>
    <li>
      Use Weka to train a nearest neighbor classifier (<b>Lazy/IB1</b>).
    <li>
      Do the same with a decision tree (<b>Trees/J48</b>).
    <li>
      Visualize the decision boundaries and explain why the MLP, 
      nearest neighbor, and decision tree classifiers work or don't work
      in this case.
  </ol>
</p>

<% end %>

You will soon find the lecture slides for this part on the <a href="https://courses.helsinki.fi/DATA15001/119123276">course homepage</a>
under Materials.


<% partial 'partials/material_heading' do %>
  Digital Signal Processing
<% end %>

<p>
  The topics of this week include two special kinds of data that
  require somewhat specific techniques: digital signals (such as audio
  or images), and natural language.
</p>

<p>
  Next week we will finish the Digital Signal Processing and Robotics
  theme by studying robotics.
</p>

<% partial 'partials/hint', locals: { name: 'Learning objectives of Part 5' } do %>

<table class="table">
  <tr>
    <td>
      Theme
    </td>
    <td>
      Objectives (after the course, you ...)
    </td>
  </tr>
  <tr>
    <td>
      Natural language processing
    </td>
    <td>
      <ul>
	<li>can generate sentences from a given context-free grammar
	<li>can parse a sentence using the Cocke-Younger-Kasami algorithm
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      Digital Signal Processing and Robotics
    </td>
    <td>
      <ul>
	<li>can describe the principles of at least one pattern recognition method (e.g., SIFT/SURF)
	<li>can apply at least one pattern recognition method in practice
      </ul>
    </td>
  </tr>
</table>

<% end %>

<% partial 'partials/material_sub_heading' do %>
  Digital signal representations
<% end %>

<p>
  Digital image and audio signals represent recordings of natural
  signals in numerical form. For images, the values represent 
  pixel intensities stored in a two-dimensional array. A third
  dimension can be used to store different bands for, e.g.,
  red, green, and blue (RGB). We can also think of the values
  are functions of the x and y coordinates, F(x,y).
</p>

<p>
  <i>Fun fact (but not required for the exam):</i> In the case of
  audio, the signal can be thought to be a signal of time, F(t), but
  other representations such as those that are functions of frequency,
  F(f), are commonly used. The frequency representation is
  particularly well suited for compact representation since frequency
  bands outside the audible range 20 Hz to 20 kHz can be left out
  without observable (to humans) effect.
</p>

<% partial 'partials/material_sub_heading' do %>
  Pattern recognition
<% end %>

<p>
  <b>Pattern recognition</b> is a good example of an AI problem
  involving digital signals. It means generally the problem is
  recognizing an object in a signal (typically an image). The task is
  made hard because signals tend to vary greatly depending on external
  conditions such as camera angle, distance, lighting conditions, 
  echo, noise, and differences between recording devices. Therefore
  the signal is practically always different even if it is from the
  same object.
</p>

<p>
  Based on these facts, an essential requirement for successful
  pattern recognition is the extraction of <b>invariant features</b>
  in the signal. Such features are insensitive (or as insensitive as
  possible) to variation in the external conditions, and tend to be
  similar in recordings of the same object. In the lecture slides
  there are two photographs of an Afghan girl. She was identified
  based on the unique patterns in the iris of the eye. Here the iris
  is the invariant feature, which remains unchanged also even if the
  person grows older and may have otherwise a different appearance.
  We used the iris example as a metaphor for invariance with respect
  to external conditions, but it is actually also a practical personal
  identification technique
  (see <a href="https://en.wikipedia.org/wiki/Iris_recognition">Iris
  recognition</a>, Wikipedia).
</p>

<% partial 'partials/material_sub_heading' do %>
  SIFT and SURF
<% end %>

<p>
  Good examples of commonly used feature extraction techniques for
  image data include Scale-Invariant Feature Transform (SIFT) and
  Speeded-Up Robust Features (SURF). We take a closer look at the
  SURF technique. It can be broken into two or three stages:
  <ol>
    <li>
      <b>Choosing interest points:</b> We identify a set of (x,y)
      points by maximizing the determinant of a so called Hessian
      matrix that characterizes the local intensity variation 
      around the point. The interest points tend to be located
      at "blobs" in the image. (You are not required to decode the
      details of this -- a rough idea is enough.)
    <li>
      <b>Feature descriptors:</b> The intensity variation around each
      interest point is described by a feature descriptor which is a
      64-dimensional vector (array of 64 real values).  In addition,
      the feature has a scale (size) and an orientation (dominant
      direction).
    <li>
      <b>Feature matching:</b> If the problem is to match objects
      between two images, then both of the above tasks are performed
      on both images. We then find pairs of feature descriptors
      (one extracted from image A and the other from image B) that
      are sufficiently close to each other in Euclidean distance.
  </ol>
</p>
<img src="/img/surf-example-mars.png" width=100%><br><br>

<p>
  Above is an example of detected features: interest point location,
  scale, and orientation shown by the circles, and the type of
  contrast shown by the color, i.e., dark on light background (red) or
  light on dark background (blue).  <i>Image credit:</i> NASA/Jet
  Propulsion Laboratory/University of Arizona. Image from the edge of
  the south polar residual cap on Mars.
</p>

<% partial 'partials/exercise', locals: { name: 'Pattern recognition A (1p)' } do %>

<p>
  Coming to TMC soon.
</p>

<% end %>


<% partial 'partials/exercise', locals: { name: 'Pattern recognition B (1p)' } do %>

<p>
  Coming to TMC soon.
</p>

<% end %>


<% partial 'partials/material_heading' do %>
  Natural Language Processing
<% end %>

<p>
  Language is another example of an area where AI has hard time
  considering how easy it feels to us, as humans, to understand
  natural scenes or language. This is probably in part because
  we don't appreciate the hardness of tasks that even a child
  can perform without much apparent practice. (Of course, a
  child actually practices these skills very intensively, and
  in fact, evolution has trained us for millions of years,
  preconditioning our "neural networks" to process information
  in this form.)
</p>

<p>
  The features that make language distinctive and different from
  many other data sources for which we may apply AI techniques 
  include:
  <ol>
    <li>
      Even though language follows an obvious <b>linear structure</b>
      where a piece of text has a beginning and an end, and the words
      inbetween are in a given order, it also has <b>hierarchical</b>
      structures including, e.g.,
      text&ndash;paragraph&ndash;sentence&ndash;word relations,
      where each sentence, for example, forms a separate unit.
    <li>
      Grammar constrains the possible (allowed) expressions in natural
      languages but still leaves room for ambiguity (unlike formal
      languages such as programming languages which must be 
      unambiguous).
    <li>
      Compared to digital signals such as audio or image, natural
      language data is much closer to <b>semantics</b>: the idea
      of a chair has a direct correspondence with the word 'chair'
      but any particular image of a chair will necessarily carry
      plenty of additional information such as the color, size,
      material of the chair and even the lighting conditions, the
      camera angle, etc, which are completely independent of the
      chair itself.
  </ol>
</p>

<% partial 'partials/material_sub_heading' do %>
  Formal languages and grammars
<% end %>


<p>
  Natural language processing (NLP) applies many concepts from
  theoretical computer science which are applicable to the study of
  formal languages. A formal language is generally a set of
  strings. For example, the set of valid mathematical expressions is a
  formal language that includes the
  string <code>(1+2)&times;6&ndash;3</code> but not the string
  <code>1+(+&times;5(</code>.
</p>

<% partial 'partials/material_sub_heading' do %>
  Parsing
<% end %>

<% partial 'partials/material_sub_heading' do %>
  CYK algorithm
<% end %>

<% partial 'partials/exercise', locals: { name: 'NLP: CYK algorithm (pencil-and-paper) (1p)' } do %>

<p>
  coming soon...
</p>

<% end %>

<% partial 'partials/material_sub_heading' do %>
  Parse trees and ambiguity
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Advanced topics in parsing
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Applicatins of NLP
<% end %>

<% partial 'partials/exercise', locals: { name: 'NLP: Knowledge extraction (1p)' } do %>

<p>
  You'll find a template for this exercise in TMC.</p>

<p>
  <ol>
    <li>
      Implement method <code>extractSubject</code> in
      class <code>Extractor</code>. The input will be a parse
      tree of a sentence. You should try to identify the <b>subject</b> of
      the sentence (<i>who/what</i> does something). One heuristic method
      that doesn't always work, but is close enough, is as follows:
      <ol>
	<li>Assume that the root node of the parse tree, S, has
	  children NP (noun phrase) and VP (verb phrase).  If this
	  is not the case, you can skip the sentence and return null.
	<li>Choose the child NP.
	<li>The subject can be assumed to a noun in the NP subtree
	  (see example below). You can identify nouns by the POS-tag
	  which should be either NN, NNP, NNPS, or NNS.
	<li>Use breadth-first search to find the noun closest to the root.<br>
	  <img src="/img/exercises/ex5/parsetree.png" width=52%>
      </ol>
<br><br>

      
      The TMC template has tests to verify your solution.<br><br>
    <li>
      Next, implement the following logic in method <code>main</code>
      in class <code>Main</code> using the tools available in class
      <code>NLPUtils</code>: The template has ready-made functionality
      for iterating through all sentences in Franz Kafka's
      <i>The Metamorphosis</i>. Reject all sentences that don't contain
      the word "Gregor".<br><br>
    <li>
      For all remaining sentences, produce a 
      parsing tree. (If the parsing fails, reject the sentence.) Use the
      method you implemented in item 1 to identify the subject of the
      sentence. Print out all sentences where the subject is "Gregor".
      <br><br>
    <li>
      Use the output to find out what Gregor does in a similar spirit
      as this <a href="http://hint.fm/seer/#left=google%20is&right=artificial%20intelligence%20is">cool applet</a>. (No, you <i>don't</i> have to make
      a visualization!)
  </ol>
</p>

<p>
  <i>Hint:</i> In item 2, you can use method <code>contains</code>.

</p>  

<% end %>
