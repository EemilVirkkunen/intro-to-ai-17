---
  title: Part 4
  exercise_page: true
  quiz_page: false
  published: true
---




<% partial 'partials/exercise', locals: { name: 'Bayesian networks (pencil-and-paper) (1p)' } do %>

<p>
  In Part 3, we briefly discussed how Baesian networks lead to compact
  representation of probabilistic models.
  <ol>
    <li>
      Consider a model with N=4 variables, each of which has k possible
      values (0,1,...,k&ndash;1). How many different elementary events are there
      in the domain? One less is the number of probabilities that we'd have
      to determine in order to fully specify the model. (For example, in the
      Car model of Part 3, with N=6 and k=2, this number was 63.)
    <li> 
      How many parameters would we have to define to specify the model using
      an "empty" Bayesian network, i.e, a network with no edges at all?
    <li>
      What is the maximum number of parameters we'd have to define in a
      Bayesian network in this domain (N variables with k values each)?
      Remember that the graph has to be a DAG, i.e., no directed cycles are
      allowed.
    <li>
      <i>Optional:</i> Generalize to arbitrary values of N. It may be helpful to
      recall the sum of a geometric series 1+k+k<sup>2</sup>+...+k<sup>N&ndash;1</sup> =
      (k<sup>N</sup>&ndash;1)/(k&ndash;1), where k&ne;1.
  </ol>
</p>

<% end %>

<% partial 'partials/exercise', locals: { name: 'Bayesian networks: Car (programming) (1p)' } do %>

<p>
  Implement an algorithm for generating data from the Car network
  discussed in Part 3.  You should generate n tuples, each of which is
  a six element list of bits (or Boolean values). You should start by
  choosing the value of B so that it takes value 1
  (or <code>True</code>) with probability 0.9. After choosing the
  value of B, choose the value of R. If B=0, then R=0 with probability
  1. If B=1, then R=0 with probability 0.1. Continue this way,
  choosing the probabilities from the CPTs, until you have generated a
  complete tuple.
</p>

<p>
  Generate a sample of n=100 000 tuples. Use it to approximate the following
  conditional probabilities:
  <ol>
    <li>
      P(B | R,G,&not;S)
    <li>
      P(S | R,I,G)
    <li>
      P(S | &not;R,I,G)
  </ol>
  <strong>
    and (don't forget this part):
  </strong>
  <ol start=4><br>
    <li>
      give an interpretation to the above probabilities. Are they in line with
      your intuition? In other words, do they make sense?
  </ol>
</p>

<% end %>

<% partial 'partials/exercise', locals: { name: 'Bayesian networks: Earthquake (programming) (1p)' } do %>

<p>
  Consider the following scenario. You live in an area where
  earthquakes happen on the average every 111th day. Another annoying
  thing is that a burglar breaks into your home on the average once a
  year. Both events occur independently of each other and uniformly
  throughout the year (so any day is like another).
</p>

<p>
  In case of an earthquake, your home alarm goes off with probability 0.81.
  If your home is broken into, the alarm goes off with probability 0.92.
  If these both should occur at the same time, the alarm goes off with
  probability 0.97. If nothing special is going on, there's a false alarm
  with probability 0.0095.
</p>

<p>
  Now modify the algorithm from the previous exercise so that you can generate
  data from variables [E]arthquake, [B]urglar, [A]larm. Generate a sample of 
  n=100 000 tuples.
  <ol>
    <li>
      Among the tuples with A=1, what is the fraction where B=1?
    <li>
      Among the tuples with A=1 and E=1, what is the fraction where B=1?
  </ol>
  <strong>
    and (don't forget these):
  </strong>
  <ol start=3><br>
    <li>
      give an interpretation to the above probabilities. Are they in line with
      your intuition? In other words, do they make sense? In particular:
      which of the fractions is bigger and why?
    </li>
    <li>
      Repeat the experiment a couple of times to get a feeling of how much
      the results change just by chance. (This is the nature of the Monte
      Carlo approximation.) Experiment with different values of n. How does
      it affect the variability?
  </ol>
</p>

<% end %>


<% partial 'partials/material_heading' do %>
  Machine Learning
<% end %>

<p>
</p>

<% partial 'partials/hint', locals: { name: 'Learning objectives of Part 4' } do %>

<table class="table">
  <tr>
    <td>
      Theme
    </td>
    <td>
      Objectives (after the course, you ...)
    </td>
  </tr>
    <td>
      Machine learning
    </td>
    <td>
      <ul>
	<li>can distinguish between unsupervised and supervised machine learning scenarios
	<li>can implement at least two supervised classification methods (e.g., naive Bayes, nearest neighbour classifier)
	<li>know the main types of neural networks (feed-forward, recurrent, self-organizing map) and their main principles
	<li>can implement the perceptron algorithm in a simple binary classification problem
      </ul>
    </td>
  </tr>
</table>

<% end %>

<% partial 'partials/material_sub_heading' do %>
  Kinds of Machine Learning
<% end %>

<p>
  <ol>
    <li>Supervised learning
    <li>Unsupervised learning
    <li>Reinforcement learning
  </ol>
</p>

<% partial 'partials/material_sub_heading' do %>
  k-Nearest Neighbor (k-NN) Classifier
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Naive Bayes Classifier
<% end %>


<% partial 'partials/material_heading' do %>
  Neural Networks
<% end %>

<% partial 'partials/material_sub_heading' do %>
  What is Special About Neural Networks?
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Weights and Activation
<% end %>

<% partial 'partials/material_sub_heading' do %>
  The Perceptron Algorithm
<% end %>

<% partial 'partials/material_sub_heading' do %>
  Recurrent Neural Networks
<% end %>

<% partial 'partials/material_sub_heading' do %>
  The Self-Organizing Map (SOM)
<% end %>

